Line 1: Statistics is the study and manipulation of data, including ways to gather, review, analyze, and draw conclusions from data. The two major areas of statistics are descriptive and inferential statistics. Statistics can be used to make better-informed business and investing decisions.
Line 2: Statistics is a collection of methods for collecting, displaying, analyzing, and drawing conclusions from data. Definition: Descriptive statistics. Descriptive statistics is the branch of statistics that involves organizing, displaying, and describing data.
Line 3: Statistics is the practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample.
Line 4: Statistics is a branch of applied mathematics that involves the collection, description, analysis, and inference of conclusions from quantitative data. The mathematical theories behind statistics rely heavily on differential and integral calculus, linear algebra, and probability theory.
Line 5: Sampling error is very much the concern of the statistician, who imagines that the group of people in the study is just one of the many possible samples from the population of interest. Despite it being widely condemned,1 the dominant way of summarising the evidence from a research study is by the P value. It should be obvious that the evidence from a research study cannot reasonably be summarised as just a single number, but the use of P values remains unshakeable. Further, the practice of labelling P values as significant or not significant leads not only to dichotomous decisions but often also to the belief that the research question has been answered.
Line 6: Interpretation of a study’s results should be primarily based on the estimated effect and a measure of its uncertainty. In mainstream statistics, the uncertainty of estimates is indicated by the use of confidence intervals. Before the mid-1980s, confidence intervals were rarely seen in clinical research articles. Around 1986 things changed, and these days almost all clinical research articles in major journals include confidence intervals. The confidence interval is a range of uncertainty around the estimate of interest, such as the treatment effect in a controlled trial. 
Line 7: A common mistake is to believe that the confidence interval expresses all the uncertainty. Rather, the confidence interval expressed uncertainty from just one cause—namely the uncertainty due to having taken a sample from the population defined by the inclusion criteria. Often there are other sources of uncertainty that may be even more important to consider, in particular relating to possibly biased results. We address these in our linked statistics note.
Line 8: Statistical analysis of research results mainly uses confidence intervals and hypothesis tests to capture the uncertainty rising from our study being on a sample of participants drawn from a much larger population, in which our interest mainly lies. But beyond the issue of sampling variation there are other sources of uncertainty that may be even more important to consider. In measurement, a distinction is made between precision, which is how variable are measurements on the same person by the same method made at the same time, and accuracy, which is how close the measurement is to what we actually want to know. For example, if we were to ask a group of patients on two occasions how much alcohol they typically consume, this would enable us to estimate precision, how repeatable answers are, but not how close these answers are to how much they actually drink, which we might suspect to be higher. In the same way, a confidence interval tells us about the precision of research results, what would happen if we were to repeat the same study, not their accuracy, which is how close the study is to the truth.
Line 9: In general, beyond the imprecision or uncertainty of numerical results arising from sampling, the main concern is the possibility that the study results are biased. Recent developments in appraising published randomised trials have switched from considering “quality” (essentially undefinable) to assessing explicitly the risk of bias in relation to the way the study was done. Here sources of possible bias include lack of blinding and losses to follow up (missing data). But bias is especially relevant in non-randomised studies, where there will be major concerns about possible confounding, where an apparent relationship between two things may be the result of the relationships of both to a third.
Line 10: Large numbers, increasingly common in this era of big data, will produce narrow confidence intervals. These can create an illusion of accuracy, but they ignore all sources of possible bias that are not affected by sample size, and so these other sources become relatively more important. A recent example of a very precise but seriously wrong answer purported to show that skin cancer was protective for heart attacks and all-cause mortality. So, although confidence intervals are a valuable way of depicting uncertainty, they are always too narrow in the sense that they reflect only statistical uncertainty, precision rather than accuracy.
Line 11: The methods described can be applied in a wide range of settings, including the results from meta-analysis and regression analyses. The main context where they are not correct is small samples where the outcome is continuous and the analysis has been done by a t test or analysis of variance, or the outcome is dichotomous and an exact method has been used for the confidence interval.
Line 12: The clever idea behind the bootstrap is to create multiple datasets from the real dataset without needing to make any assumptions. Our observed sample is representative of a population about which we wish to make inferences, so a set of randomly chosen observations from our sample will be equally representative of the original population. We can generate a sample of the same size as the original data set by randomly choosing real observations one at a time. Each observation has an equal chance of being chosen each time, so some observations will be picked more than once and some won’t be picked at all. That doesn’t matter; the new “bootstrap” sample is comparable to the original data set and is equally representative of the target population.
Line 13: This note gives the general idea of the bootstrap; there are many variations.4 We can get a bootstrap estimate for any quantity we can calculate from any sample. Bootstrap methods are particularly favoured by health economists, because cost data tend to be highly skewed and unsuited to conventional approaches. They are also useful for complex datasets—for example, when the observations aren’t independent.
Line 14: An important aspect of a sampling distribution is the standard error (SE). The standard error is the standard deviation of a sampling distribution.  For a single categorical variable this may be referred to as the standard error of the proportion. For a single quantitative variable this may be referred to as the standard error of the mean. If a sampling distribution is constructed using data from a population, the mean of the sampling distribution will be approximately equal to the population parameter.
Line 15: The process of constructing a sampling distribution from a known population is the same for all types of parameters (i.e., one group proportion, one group mean, difference in two proportions, difference in two means, simple linear regression slope, and correlation). In each case we take a simple random sample of from the population without replacement, record the sample statistic of interest, return those observations back into the population, and repeat many times. That distribution of sample statistics is known as the sampling distribution. If the sample size is large, the sampling distribution will be approximately normally with a mean equal to the population parameter.
Line 16: These are all approximately normally distributed. You were first introduced to the normal distribution as a special type of symmetrical distribution. In this lesson, we'll review normal distributions, learn how to use Minitab to construct plots of normal distributions, and learn how the Central Limit Theorem allows us to apply what we know about the normal distribution to construct confidence intervals and conduct hypothesis tests without using simulations.
Line 17: While we cannot determine the probability for any one given value because the distribution is continuous, we can determine the probability for a given interval of values. The probability for an interval is equal to the area under the density curve. The total area under the curve is 1.00, or 100%. In other words, 100% of observations fall under the curve.
Line 18: Most statistical methods, such as t tests, are called parametric because they estimate parameters of some underlying theoretical distribution. Non-parametric methods, such as the Mann-Whitney U test and the log rank test for survival data, do not assume any particular family for the distribution of the data and so do not estimate any parameters for such a distribution.
Line 19: In some contexts parameters are values that can be altered to see what happens to the performance of some system. For example, the performance of a screening programme (such as positive predictive value or cost effectiveness) will depend on aspects such as the sensitivity and specificity ofthe screening test. If we look to see how the performance would change if, say, sensitivity and specificity were improved, then we are treating these as parameters rather than using the values observed in a real set of data.
Line 20: Bayesian methods are based on the idea that unknown quantities, such as population means and proportions, have probabilitydistributions. The probability distribution for a population proportionexpresses our prior knowledge or belief about it, before we add the knowledge which comes from our data.
Line 21: Frequentist methods regard the population value as a fixed, unvarying (but unknown) quantity, without a probability distribution. Frequentists then calculate confidence intervals for this quantity, or significance tests of hypotheses concerning it. Bayesians reasonably object that this does not allow us to use our wider knowledge of the problem. Also, it does not provide what researchers seem to want, which is to be able to say that there is a probability of 95% that the population value lies within the 95% confidence interval, or that the probability that the null hypothesis is true is less than 5%. It is argued that researchers want this, which is why they persistently misinterpret confidence intervals and significance tests in this way.
Line 22: A major difficulty, of course, is deciding on the prior distribution. This is going to influence the conclusions of the study, yet it may be a subjective synthesis of the available information, so the same data analysed by different investigators could lead to different conclusions. Another difficulty is that Bayesian methods may lead to intractable computational problems. (All widely available statistical packages use frequentist methods.)
Line 23: The advent of very powerful computers has given a new impetus to the Bayesians. Computer intensive methods of analysis are being developed, which allow new approaches to very difficult statistical problems, such as the location of geographical clusters of cases of a disease. This new practicability of the Bayesian approach is leading to a change in the statistical paradigm—and a rapprochement between Bayesians and frequentists.
Line 24: This type of analysis can be extended to more complex data sets with two classifying variables, using two way analysis of variance, and so on. Analysis of variance is a special type of regression analysis, and most data sets for which analysis of variance is appropriate can be analysed by regression with the same results.
Line 25: In this context the name “normal” causes much confusion. In statistics it is just a name; statisticians often use a capital N to emphasise this and to clarify that Normality does not necessarily imply normality. Indeed, in some medical specialties normal distributions are rare. Various methods of analysis make assumptions about normality, including correlation, regression, t tests, and analysis of variance. It is not in fact necessary for the distribution of the observed data to be normal, but rather the sample values should be compatible with the population (which they represent) having a normal distribution. Indeed, samples from a population in which the true distribution is normal will not necessarily look normal themselves, especially if the sample is small.
Line 26: We can consider binary attributes in the same way. For example, the proportions of individuals with asthma will vary from sample to sample. If having asthma is represented by the value 1 and not having asthma by the value 0 then the mean of these values in the sample is the proportion of individuals with asthma. Thus a proportion is also a mean and will follow a normal distribution. These methods are not valid in small samples—some “exact” methods can be used. Similar comments apply to some other statistics, such as regression coefficients or standardised mortality ratios, but for mortality ratios the sample size may have to be very large indeed.
Line 27: We have often come across the idea that we should not use t distribution methods for small samples but should instead use rank based methods. The statement is sometimes that we should not use t methods at all for samples of fewer than six observations. But, as we noted, rank based methods cannot produce anything useful for such small samples. The aversion to parametric methods for small samples may arise from the inability to assess the distribution shape when there are so few observations. How can we tell whether data follow a normal distribution if we have only a few observations? The answer is that we have not only the data to be analysed, but usually also experience of other sets of measurements of the same thing. In addition, general experience tells us that body size measurements are usually approximately normal, as are the logarithms of many blood concentrations and the square roots of counts.
Line 28: The standard deviation of natural log transformed data is also problematic—what does it mean? By the same token it is a fractional standard deviation, similar to the standard deviation divided by the mean—that is, a form of coefficient of variation. Multiplying by 100 converts the log standard deviation to a coefficient of variation in percentage units.
Line 29: We will introduce important concepts such as random variables, independence, Monte Carlo simulations, expected values, standard errors, and the Central Limit Theorem. These statistical concepts are fundamental to conducting statistical tests on data and understanding whether the data you are analyzing is likely occurring due to an experimental method or to chance.
Line 30: For anyone taking first steps in data science, Probability is a must know concept. Concepts of probability theory are the backbone of many important concepts in data science like inferential statistics to Bayesian networks. It would not be wrong to say that the journey of mastering statistics begins with probability.
Line 31: Standard deviation measures the variation or dispersion of the data points in a dataset. It depicts the closeness of the data point to the mean and is calculated as the square root of the variance. In data science, the standard deviation is usually used to identify the outliers in a data set. The data points which lie one standard deviation away from the mean are considered to be unusual.
Line 32: The topics of statistics and probability covered in the article are really important but there are many other topics such as Probability Distribution Functions and their types, Covariance, and Correlation, etc. that have not been covered here because they require separate attention due to their graphical nature. Mathematics and statistics are the heart of data science. The topics covered in this article are the basis of many algorithms, error calculating formulas, and graphical understanding of things, thus are very important and cannot be ignored.
Line 33: Probability theory, a branch of mathematics concerned with the analysis of random phenomena. The outcome of a random event cannot be determined before it occurs, but it may be any one of several possible outcomes. The actual outcome is considered to be determined by chance.
Line 34: The expectation of a random variable is a number that attempts to capture the center of that random variable's distribution. It can be interpreted as the long-run average of many independent samples from the given distribution.
Line 35: Whereas expectation provides a measure of centrality, the variance of a random variable quantifies the spread of that random variable's distribution. The variance is the average value of the squared difference between the random variable and its expectation.
Line 36: Sabermetrics or SABRmetrics is the empirical analysis of baseball, especially baseball statistics that measure in-game activity. Sabermetricians collect and summarize the relevant data from this in-game activity to answer specific questions.
Line 37: In the past several decades, the baseball industry has become more enlightened -- thanks to an assist from advanced metrics. Although standard statistics remain quite valuable, advanced formulas and figures have played a pivotal role in the creation of championship teams -- both in Major League Baseball and fantasy leagues around the world. Today, each big league franchise relies upon advanced stats to some degree, with a growing number of clubs employing complete staffs devoted to their study, development and deployment in decision-making processes.
Line 38: Baseball is perhaps the most numbers oriented of the major American sports. With 162 regular season games, there are few sports with as much sample size as baseball. Baseball statistics are truly relevant in ways that simply do not hold up for other games. So what does a baseball statistician do? Finds relevant numbers and looks for trends in data.
Line 39: To become a baseball data analyst, you typically need a bachelor's degree in math, statistics, or a related field. Sports Management Worldwide offers eight-week courses on sports analytics, allowing you to focus on baseball and learn the software and tools currently used for professional teams. To be successful in this position, you need excellent communication skills to present your data findings to coaches and scouts and interpret how to utilize the information. Additional qualifications include a strong understanding of the game, attention to detail, and analytical thinking ability.
Line 40: The win probability for a specific situation in baseball (including the inning, number of outs, men on base, and score) is obtained by first finding all the teams that have encountered this situation. Then the winning percentage of these teams in these situations is found.


