Line 1: Statistics is the study and manipulation of data, including ways to gather, review, analyze, and draw conclusions from data. The two major areas of statistics are descriptive and inferential statistics. Statistics can be used to make better-informed business and investing decisions.
Line 2: Statistics is a collection of methods for collecting, displaying, analyzing, and drawing conclusions from data. Definition: Descriptive statistics. Descriptive statistics is the branch of statistics that involves organizing, displaying, and describing data.
Line 3: Statistics is the practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample.
Line 4: Statistics is a branch of applied mathematics that involves the collection, description, analysis, and inference of conclusions from quantitative data. The mathematical theories behind statistics rely heavily on differential and integral calculus, linear algebra, and probability theory.
Line 5: Sampling error is very much the concern of the statistician, who imagines that the group of people in the study is just one of the many possible samples from the population of interest. Despite it being widely condemned,1 the dominant way of summarising the evidence from a research study is by the P value. It should be obvious that the evidence from a research study cannot reasonably be summarised as just a single number, but the use of P values remains unshakeable. Further, the practice of labelling P values as significant or not significant leads not only to dichotomous decisions but often also to the belief that the research question has been answered.
Line 6: Interpretation of a study’s results should be primarily based on the estimated effect and a measure of its uncertainty. In mainstream statistics, the uncertainty of estimates is indicated by the use of confidence intervals. Before the mid-1980s, confidence intervals were rarely seen in clinical research articles. Around 1986 things changed, and these days almost all clinical research articles in major journals include confidence intervals. The confidence interval is a range of uncertainty around the estimate of interest, such as the treatment effect in a controlled trial. 
Line 7: A common mistake is to believe that the confidence interval expresses all the uncertainty. Rather, the confidence interval expressed uncertainty from just one cause—namely the uncertainty due to having taken a sample from the population defined by the inclusion criteria. Often there are other sources of uncertainty that may be even more important to consider, in particular relating to possibly biased results. We address these in our linked statistics note.
Line 8: Statistical analysis of research results mainly uses confidence intervals and hypothesis tests to capture the uncertainty rising from our study being on a sample of participants drawn from a much larger population, in which our interest mainly lies. But beyond the issue of sampling variation there are other sources of uncertainty that may be even more important to consider. In measurement, a distinction is made between precision, which is how variable are measurements on the same person by the same method made at the same time, and accuracy, which is how close the measurement is to what we actually want to know. For example, if we were to ask a group of patients on two occasions how much alcohol they typically consume, this would enable us to estimate precision, how repeatable answers are, but not how close these answers are to how much they actually drink, which we might suspect to be higher. In the same way, a confidence interval tells us about the precision of research results, what would happen if we were to repeat the same study, not their accuracy, which is how close the study is to the truth.
Line 9: In general, beyond the imprecision or uncertainty of numerical results arising from sampling, the main concern is the possibility that the study results are biased. Recent developments in appraising published randomised trials have switched from considering “quality” (essentially undefinable) to assessing explicitly the risk of bias in relation to the way the study was done. Here sources of possible bias include lack of blinding and losses to follow up (missing data). But bias is especially relevant in non-randomised studies, where there will be major concerns about possible confounding, where an apparent relationship between two things may be the result of the relationships of both to a third.
Line 10: Large numbers, increasingly common in this era of big data, will produce narrow confidence intervals. These can create an illusion of accuracy, but they ignore all sources of possible bias that are not affected by sample size, and so these other sources become relatively more important. A recent example of a very precise but seriously wrong answer purported to show that skin cancer was protective for heart attacks and all-cause mortality. So, although confidence intervals are a valuable way of depicting uncertainty, they are always too narrow in the sense that they reflect only statistical uncertainty, precision rather than accuracy.
Line 11: The methods described can be applied in a wide range of settings, including the results from meta-analysis and regression analyses. The main context where they are not correct is small samples where the outcome is continuous and the analysis has been done by a t test or analysis of variance, or the outcome is dichotomous and an exact method has been used for the confidence interval.
Line 12: The clever idea behind the bootstrap is to create multiple datasets from the real dataset without needing to make any assumptions. Our observed sample is representative of a population about which we wish to make inferences, so a set of randomly chosen observations from our sample will be equally representative of the original population. We can generate a sample of the same size as the original data set by randomly choosing real observations one at a time. Each observation has an equal chance of being chosen each time, so some observations will be picked more than once and some won’t be picked at all. That doesn’t matter; the new “bootstrap” sample is comparable to the original data set and is equally representative of the target population.
Line 13: This note gives the general idea of the bootstrap; there are many variations.4 We can get a bootstrap estimate for any quantity we can calculate from any sample. Bootstrap methods are particularly favoured by health economists, because cost data tend to be highly skewed and unsuited to conventional approaches. They are also useful for complex datasets—for example, when the observations aren’t independent.
Line 14: An important aspect of a sampling distribution is the standard error (SE). The standard error is the standard deviation of a sampling distribution.  For a single categorical variable this may be referred to as the standard error of the proportion. For a single quantitative variable this may be referred to as the standard error of the mean. If a sampling distribution is constructed using data from a population, the mean of the sampling distribution will be approximately equal to the population parameter.
Line 15: The process of constructing a sampling distribution from a known population is the same for all types of parameters (i.e., one group proportion, one group mean, difference in two proportions, difference in two means, simple linear regression slope, and correlation). In each case we take a simple random sample of from the population without replacement, record the sample statistic of interest, return those observations back into the population, and repeat many times. That distribution of sample statistics is known as the sampling distribution. If the sample size is large, the sampling distribution will be approximately normally with a mean equal to the population parameter.
Line 16: These are all approximately normally distributed. You were first introduced to the normal distribution as a special type of symmetrical distribution. In this lesson, we'll review normal distributions, learn how to use Minitab to construct plots of normal distributions, and learn how the Central Limit Theorem allows us to apply what we know about the normal distribution to construct confidence intervals and conduct hypothesis tests without using simulations.
Line 17: While we cannot determine the probability for any one given value because the distribution is continuous, we can determine the probability for a given interval of values. The probability for an interval is equal to the area under the density curve. The total area under the curve is 1.00, or 100%. In other words, 100% of observations fall under the curve.
Line 18: Most statistical methods, such as t tests, are called parametric because they estimate parameters of some underlying theoretical distribution. Non-parametric methods, such as the Mann-Whitney U test and the log rank test for survival data, do not assume any particular family for the distribution of the data and so do not estimate any parameters for such a distribution.
Line 19: In some contexts parameters are values that can be altered to see what happens to the performance of some system. For example, the performance of a screening programme (such as positive predictive value or cost effectiveness) will depend on aspects such as the sensitivity and specificity ofthe screening test. If we look to see how the performance would change if, say, sensitivity and specificity were improved, then we are treating these as parameters rather than using the values observed in a real set of data.
Line 20: Bayesian methods are based on the idea that unknown quantities, such as population means and proportions, have probabilitydistributions. The probability distribution for a population proportionexpresses our prior knowledge or belief about it, before we add the knowledge which comes from our data.
Line 21: Frequentist methods regard the population value as a fixed, unvarying (but unknown) quantity, without a probability distribution. Frequentists then calculate confidence intervals for this quantity, or significance tests of hypotheses concerning it. Bayesians reasonably object that this does not allow us to use our wider knowledge of the problem. Also, it does not provide what researchers seem to want, which is to be able to say that there is a probability of 95% that the population value lies within the 95% confidence interval, or that the probability that the null hypothesis is true is less than 5%. It is argued that researchers want this, which is why they persistently misinterpret confidence intervals and significance tests in this way.
Line 22: A major difficulty, of course, is deciding on the prior distribution. This is going to influence the conclusions of the study, yet it may be a subjective synthesis of the available information, so the same data analysed by different investigators could lead to different conclusions. Another difficulty is that Bayesian methods may lead to intractable computational problems. (All widely available statistical packages use frequentist methods.)
Line 23: The advent of very powerful computers has given a new impetus to the Bayesians. Computer intensive methods of analysis are being developed, which allow new approaches to very difficult statistical problems, such as the location of geographical clusters of cases of a disease. This new practicability of the Bayesian approach is leading to a change in the statistical paradigm—and a rapprochement between Bayesians and frequentists.
Line 24: This type of analysis can be extended to more complex data sets with two classifying variables, using two way analysis of variance, and so on. Analysis of variance is a special type of regression analysis, and most data sets for which analysis of variance is appropriate can be analysed by regression with the same results.
Line 25: In this context the name “normal” causes much confusion. In statistics it is just a name; statisticians often use a capital N to emphasise this and to clarify that Normality does not necessarily imply normality. Indeed, in some medical specialties normal distributions are rare. Various methods of analysis make assumptions about normality, including correlation, regression, t tests, and analysis of variance. It is not in fact necessary for the distribution of the observed data to be normal, but rather the sample values should be compatible with the population (which they represent) having a normal distribution. Indeed, samples from a population in which the true distribution is normal will not necessarily look normal themselves, especially if the sample is small.
Line 26: We can consider binary attributes in the same way. For example, the proportions of individuals with asthma will vary from sample to sample. If having asthma is represented by the value 1 and not having asthma by the value 0 then the mean of these values in the sample is the proportion of individuals with asthma. Thus a proportion is also a mean and will follow a normal distribution. These methods are not valid in small samples—some “exact” methods can be used. Similar comments apply to some other statistics, such as regression coefficients or standardised mortality ratios, but for mortality ratios the sample size may have to be very large indeed.
Line 27: We have often come across the idea that we should not use t distribution methods for small samples but should instead use rank based methods. The statement is sometimes that we should not use t methods at all for samples of fewer than six observations. But, as we noted, rank based methods cannot produce anything useful for such small samples. The aversion to parametric methods for small samples may arise from the inability to assess the distribution shape when there are so few observations. How can we tell whether data follow a normal distribution if we have only a few observations? The answer is that we have not only the data to be analysed, but usually also experience of other sets of measurements of the same thing. In addition, general experience tells us that body size measurements are usually approximately normal, as are the logarithms of many blood concentrations and the square roots of counts.
Line 28: The standard deviation of natural log transformed data is also problematic—what does it mean? By the same token it is a fractional standard deviation, similar to the standard deviation divided by the mean—that is, a form of coefficient of variation. Multiplying by 100 converts the log standard deviation to a coefficient of variation in percentage units.
Line 29: We will introduce important concepts such as random variables, independence, Monte Carlo simulations, expected values, standard errors, and the Central Limit Theorem. These statistical concepts are fundamental to conducting statistical tests on data and understanding whether the data you are analyzing is likely occurring due to an experimental method or to chance.
Line 30: For anyone taking first steps in data science, Probability is a must know concept. Concepts of probability theory are the backbone of many important concepts in data science like inferential statistics to Bayesian networks. It would not be wrong to say that the journey of mastering statistics begins with probability.
Line 31: Standard deviation measures the variation or dispersion of the data points in a dataset. It depicts the closeness of the data point to the mean and is calculated as the square root of the variance. In data science, the standard deviation is usually used to identify the outliers in a data set. The data points which lie one standard deviation away from the mean are considered to be unusual.
Line 32: The topics of statistics and probability covered in the article are really important but there are many other topics such as Probability Distribution Functions and their types, Covariance, and Correlation, etc. that have not been covered here because they require separate attention due to their graphical nature. Mathematics and statistics are the heart of data science. The topics covered in this article are the basis of many algorithms, error calculating formulas, and graphical understanding of things, thus are very important and cannot be ignored.
Line 33: Probability theory, a branch of mathematics concerned with the analysis of random phenomena. The outcome of a random event cannot be determined before it occurs, but it may be any one of several possible outcomes. The actual outcome is considered to be determined by chance.
Line 34: The expectation of a random variable is a number that attempts to capture the center of that random variable's distribution. It can be interpreted as the long-run average of many independent samples from the given distribution.
Line 35: Whereas expectation provides a measure of centrality, the variance of a random variable quantifies the spread of that random variable's distribution. The variance is the average value of the squared difference between the random variable and its expectation.
Line 36: Sabermetrics or SABRmetrics is the empirical analysis of baseball, especially baseball statistics that measure in-game activity. Sabermetricians collect and summarize the relevant data from this in-game activity to answer specific questions.
Line 37: In the past several decades, the baseball industry has become more enlightened -- thanks to an assist from advanced metrics. Although standard statistics remain quite valuable, advanced formulas and figures have played a pivotal role in the creation of championship teams -- both in Major League Baseball and fantasy leagues around the world. Today, each big league franchise relies upon advanced stats to some degree, with a growing number of clubs employing complete staffs devoted to their study, development and deployment in decision-making processes.
Line 38: Baseball is perhaps the most numbers oriented of the major American sports. With 162 regular season games, there are few sports with as much sample size as baseball. Baseball statistics are truly relevant in ways that simply do not hold up for other games. So what does a baseball statistician do? Finds relevant numbers and looks for trends in data.
Line 39: To become a baseball data analyst, you typically need a bachelor's degree in math, statistics, or a related field. Sports Management Worldwide offers eight-week courses on sports analytics, allowing you to focus on baseball and learn the software and tools currently used for professional teams. To be successful in this position, you need excellent communication skills to present your data findings to coaches and scouts and interpret how to utilize the information. Additional qualifications include a strong understanding of the game, attention to detail, and analytical thinking ability.
Line 40: The win probability for a specific situation in baseball (including the inning, number of outs, men on base, and score) is obtained by first finding all the teams that have encountered this situation. Then the winning percentage of these teams in these situations is found.
Line 41: Probability and statistics calculations are just one input. Resampling simulation enables us to get past issues of mathematical technique and focus on the cru-cial statistical elements of statistical problems. If you intend to go on to advanced statistical work, the older standard method can be learned alongside resampling methods. Your introduction to the conventional method may thereby be made much more meaningful.
Line 42: The central concept for dealing with uncertainty is probability. Hence we must inquire into the “meaning” of the termprobability. You have been using the notion of probability all your life when drawing conclusions about what you expect to happen, and in reaching decisions in your public and personal lives. You act on the basis of probabilities.
Line 43: The common meaning of the term “probability” is as follows: Any particular stated probability is an assertion that indicates how likely you believe it is that an event will occur.
Line 44: The probability of an event can be said to be the proportion of times that the event has taken place in the past, usually based on a long series of trials. Insurance companies use this when they estimate the probability that a thirty-five-year-old postman will die during a period for which he wants to buy an insurance policy. (Notice this shortcoming: Sometimes you must bet upon events that have never or only infrequently taken place before, and so you cannot reasonably reckon the proportion of times it occured.
Line 45: The probability that an event will take place or that a statement is true can be said to correspond to the odds at which you would bet that the event will take place. (Notice a shortcoming of this concept: You might be willing to accept a five-dollar bet at 2-1 odds that your team will win the game, but you might be unwilling to bet a hundred dollars at the same odds.)
Line 46: The idea of probability arises when you are not sure about what will happen in an uncertain situation—that is, when you lack information and therefore can only make an estimate. For example, if someone asks you your name, you do not use the concept of probability to answer; you know the answer to a very high degree of surety. To be sure, there is some chance that you do not know your own name, but for all practical purposes you can be quite sure of the answer. If someone asks you who will win tomorrow’s ball game, however, there is a considerable chance that you will be wrong no matter what you say. Whenever there is a reasonable chance that your prediction will be wrong, the concept of probability can help you.
Line 47: Sometimes one knows a probability, such as in the case of a gambler playing black on an honest roulette wheel, or an insurance company issuing a policy on an event with which it has had a lot of experience, such as a life insurance policy. But often one does not know the probability of a future event. Therefore, our concept of probability must include situations where extensive data are not available.
Line 48: All of the many techniques used to estimate probabilities should be thought of as proxies for the actual probability. For example, if Mission Control at Space Central simulates what should and probably will happen in space if a valve is turned aboard a space craft just now being built, the test result on the ground is a proxy for the real probability of what will happen in space.
Line 49: The conceptual probability in any specific situation is an interpretation of all the evidence that is then available. For example, a wise biomedical worker’s estimate of the chance that a given therapy will have a positive effect on a sick patient should be an interpretation of the results of not just one study in isolation, but of the results of that study plus everything else that is known about the disease and the therapy. A wise policymaker in business, government, or the military will base a probability estimate on a wide variety of information and knowledge. The same is even true of an insurance underwriter who bases a life-insurance or shipping-insurance rate not only on extensive tables of long-time experience but also on recent knowledge of other kinds. The choice of a method of estimating a probability constitutes an operational definition.
Line 50: How does one estimate a probability in practice? This involves practical skills not very different from the practical skills required to estimate with accuracy the length of a golf shot, the number of carpenters you will need to build a house, or the time it will take you to walk to a friend’s house; we will consider elsewhere some ways to improve your practical skills in estimating probabilities. For now, let us simply categorize and consider in the next section various ways of estimating an ordinary garden variety of probability, which is called an “unconditional” probability.
Line 51: The first possible source for an estimate of the probability of drawing an even-numbered spade is the purely empirical method of experience. If you have watched card games casually from time to time, you might simply guess at the proportion of times you have experienced.
Line 52: Observation of repeated events can help you estimate the probability that a machine will turn out a defective part or that a child can memorize four nonsense syllables correctly in one attempt. You watch repeated trials of similar events and record the results.
Line 53: In view of the necessarily judgmental aspects of probability estimates, the reader may prefer to talk about “degrees of belief” instead of probabilities. That’s fine, just as long as it is understood that we operate with degrees of belief in exactly the same way as we operate with probabilities; the two terms are working synonyms.
Line 54: The concept of a probability based on a frequency series can be rendered meaningless when all the observations are repetitions of a single magnitude—for example, the case of all successes and zero failures of space-shuttle launches prior to the Challenger shuttle tragedy in the 1980s; in those data alone there was no basis to estimate the probability of a shuttle failure. (Probabilists have made some rather peculiar attempts over the centuries to estimate probabilities from the length of a zero-defect time series—such as the fact that the sun has never failed to rise (foggy days aside!)—based on the undeniable fact that the longer such a series is, the smaller the probability of a failure. However, one surely has more information on which to act when one has a long series of observations of the same magnitude rather than a short series).
Line 55: A possible source of probability estimates is empirical scientific investigation with repeated trials of the phenomenon. This is an empirical method even when the empirical trials are simulations. The resampling approach to statistics produces estimates of most probabilities with this sort of experimental “Monte Carlo” method.
Line 56: A source of probability estimates is counting the possibilities, the quintessential theoretical method. Estimating probabilities by counting the possibilities has two requirements: 1) that the possibilities all be known (and therefore limited), and few enough to be studied easily; and 2) that the probability of each particular possibility be known.
Line 57: It is possible to estimate probabilities with mathematical calculation only if one knows by other means the probabilities of some related events.
Line 58: The concept of probability varies from one field of endeavor to another; it is different in the law, in science, and in business. The concept is most straightforward in decision-making situations such as business and gambling; there it is crystal-clear that one’s interest is entirely in making accurate predictions so as to advance the interests of oneself and one’s group. The concept is most difficult in social science, where there is considerable doubt about the aims and values of an investigation. In sum, one should not think of what a probability “is” but rather how best to estimate it. In practice, neither in actual decision-making situations nor in scientific work—nor in classes—do people experience difficulties estimating probabilities because of philosophical confusions. Only philosophers and mathematicians worry—and even they really do not need to worry—about the “meaning” of probability.
Line 59: To evaluate the soundness of a wager or investment, then, we need to look not at its worth after the fact—its final value, we might say—but rather at the value we can reasonably expect it to have in the future, based on what we know at the time the decision is made. We’ll call this the expected value (sometimes called expectation value). To calculate the expected value of a wager or investment, we must take into consideration: the various possible ways in which the future might turn out that are relevant to our bet, the value of our investment in those various circumstances, and the probabilities that these various circumstances will come to pass.
Line 60: It is often possible for the decision maker to know enough about the future states of nature to assign probabilities to their occurrence. Given that probabilities can be assigned, several decision criteria are available to aid the decision maker. We will consider two of these criteria: expected value and expected opportunity loss (although several others, including the maximum likelihood criterion , are available).
Line 61: Machine Intelligence is the digitization of cognitive functions that humans do, now machines are able to perform. The intelligence happens to come from a machine and not a human.
Line 62: A machine intelligence platform can harnesses human collective intelligence to bring you powerful insights and accurate models using AI, machine learning, advanced analytics and statistics.
Line 63: When making a decision, resist the urge to associate poor outcomes with bad decisions. As we make continued investments in data science and analytics, we will tend to use that data for "resulting" rather than supporting the quality of decisions, and we'll end up with many fewer aggressive or game-changing decisions. Therefore, look at decisions on the basis of how they were made rather than how they turned out -- you can win with a poor decision and lose with a good one but in the long run, it's the decision-making process that counts.
Line 64: Some decision paths have hysteresis - even if you end up at the same outcome, the path you take to get there may be different and therefore your valuation of the decision outcome is different.
Line 65: We have to imagine the future impacts of our decisions, which involves scenario planning, careful consideration of risks and future inputs (information) we may or may not see, and some of that future-proofing involves changing our reward valuation such that we are able to break consistently bad or ill-informed decision making processes.
Line 66: Decisions are a probablistic process. You need to stop thinking in certainties and recognize probabilities. Stop imagining future situations as either-or and acknowledge that most forecasts lie along continuums.
Line 67: What are the decisions we want to affect with this data? If a bank putting a lot of effort in getting reports from their branch offices, but can't recall a single instance where those report had changed their decision. That effort could have been used better.
Line 68: A measurement is an observation that quantitatively reduces uncertainty. Measurements might not yield precise, certain judgments, but they do reduce your uncertainty. Uncertainty is the lack of certainty: the true outcome, state or value is not known. Risk is a state of uncertainty in which some of the possibilities involve a loss.
Line 69: Define a decision problem and the relevant variables. Start with the decision you need to make, then figure out which variables would make your decision easier if you had better estimates of their values. Quantify your uncertainty about those variables in terms of ranges and probabilities.
Line 70: Once you determine what you know about the uncertainties involved, how can you use that information to determine what you know about the risks involved? The simplest tool for measuring such risks accurately is the Monte Carlo (MC) simulation, which can be run by Excel and many other programs.
Line 71: When measuring risk, we don’t just want to know the “average” risk or benefit. We want to know the probability of a huge loss, the probability of a small loss, the probability of a huge savings, and so on. That’s what Monte Carlo can tell us.
Line 72: An Monte Carlo simulation uses a computer to randomly generate thousands of possible values for each variable, based on the ranges we’ve estimated. The computer then calculates the outcome (in this case, the annual savings) for each generated combination of values, and we’re able to see how often different kinds of outcomes occur. To run an Monte Carlo simulation we need not just the 90% CI for each variable but also the shape of each distribution. In many cases, the normal distribution will work just fine.
Line 73: The simulation concept can (and in high-value cases should) be carried beyond this simple Monte Carlo simulation. The first step is to learn how to use a greater variety of distributions in Monte Carlo simulations. The second step is to deal with correlated (rather than independent) variables by generating correlated random numbers or by modeling what the variables have in common. A more complicated step is to use a Markov simulation, in which the simulated scenario is divided into many time intervals. This is often used to model stock prices, the weather, and complex manufacturing or construction projects. Another more complicated step is to use an agent-based model, in which independently-acting agents are simulated. 
Line 74: When you’re uncertain about a decision, this means there’s a chance you’ll make a non-optimal choice. The cost of a “wrong” decision is the difference between the wrong choice and the choice you would have made with perfect information. But it’s too costly to acquire perfect information, so instead we’d like to know which decision-relevant variables are the most valuable to measure more precisely, so we can decide which measurements to make.
Line 75: You have defined a variable you want to measure in terms of the decision it affects and how you observe it, you’ve quantified your uncertainty about it, and you’ve calculated the value of gaining additional information about it. Now it’s time to reduce your uncertainty about the variable – that is, to measure it.
Line 76: Scientists distinguish two types of measurement error: systemic and random. Random errors are random variations from one observation to the next. They can’t be individually predicted, but they fall into patterns that can be accounted for with the laws of probability. Systemic errors, in contrast, are consistent. For example, the sales staff may routinely overestimate the next quarter’s revenue by 50% (on average).
Line 77: You must also distinguish precision and accuracy. A “precise” measurement tool has low random error. E.g. if a bathroom scale gives the exact same displayed weight every time we set a particular book on it, then the scale has high precision. An “accurate” measurement tool has low systemic error. The bathroom scale, while precise, might be inaccurate if the weight displayed is systemically biased in one direction – say, eight pounds too heavy. A measurement tool can also have low precision but good accuracy, if it gives inconsistent measurements but they average to the true value.
Line 78: Systemic error is also called a “bias.” Confirmation bias: people see what they want to see. Selection bias: your sample might not be representative of the group you’re trying to measure. Observer bias: the very act of observation can affect what you observe.
Line 79: In most cases, we’ll estimate the values in a population by measuring the values in a small sample from that population. Therefore, a very small sample can often offer large reductions in uncertainty. There are a variety of tools we can use to build our estimates from small samples, and which one we should use often depends on how outliers are distributed in the population. In some cases, outliers are very close to the mean, and thus our estimate of the mean can converge quickly on the true mean as we look at new samples. In other cases, outliers can be several orders of magnitude away from the mean, and our estimate converges very slowly or not at all.
Line 80: When working with a quickly converging phenomenon and a symmetric distribution (uniform, normal, camel-back, or bow-tie) for the population, you can use the t-statistic to develop a 90% CI even when working with very small samples.
Line 81: For many decisions, one decision is required if a value is above some threshold, and another decision is required if that value is below the threshold. For such decisions, you don’t care as much about a measurement that reduces uncertainty in general as you do about a measurement that tells you which decision to make based on the threshold. 
Line 82: You can compute the value of additional information by knowing the “threshold” of the measurement where it begins to make a difference compared to your existing uncertainty.
